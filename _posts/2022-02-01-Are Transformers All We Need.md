---
layout: single
author_profile: false
---

## Impact of Transformers on NLP (and ML more broadly)
![alt text](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/Screenshot%20from%202022-02-01%2021-51-03.png)

## From Recurrence (RNNs) to Attention-Based NLP Models  
Issues with recurrent models:  
- Linear interaction distance
- Lack of parallelizability

If not recurrent, then what? How about attention?
Attention treats each word's representation as a query to access and incorporate information from a set of values. 
- attention from decoder to encoder
- self-attention is encoder-encoder or decoder-decoder where each words attends to each other word within the input or output.  

![alt text](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/Screenshot%20from%202022-02-01%2022-09-55.png) 

## Understanding the Transformer Model
![alt text](https://raw.githubusercontent.com/JingchaoZhang/JingchaoZhang.github.io/master/images/Screenshot%20from%202022-02-01%2022-26-46.png)

## [Reference](http://web.stanford.edu/class/cs224n/slides/cs224n-lecture-09-anna-goldie-2022-02-01.pdf)
